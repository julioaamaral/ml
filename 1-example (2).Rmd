---
title: "Neural Net"
output:
  pdf_document: default 
  html_document: default
---

# Hands On - Neural Networks ! 

 A neural network is a model characterized by an activation function, which is used by interconnected information processing units to transform input into output. A neural network has always been compared to human nervous system. Information in passed through interconnected units analogous to information passage through neurons in humans.  

The first layer of the neural network receives the raw input, processes it and passes the processed information to the hidden layers. The hidden layer passes the information to the last layer, which produces the output. The advantage of neural network is that it is adaptive in nature. It learns from the information provided, i.e. trains itself from the data, which has a known outcome and optimizes its weights for a better prediction in situations with unknown outcome. 

Neural network becomes handy to infer meaning and detect patterns from complex data sets.Neural network is considered as one of the most useful technique in the world of data analytics. 

A perceptron, viz. single layer neural network, is the most basic form of a neural network.  A perceptron receives multidimensional input and processes it using a weighted summation and an activation function. It is trained using a labeled data and learning algorithm that optimize the weights in the summation processor. A major limitation of perceptron model is its inability to deal with non-linearity. A multilayered neural network overcomes this limitation and helps solve non-linear problems. The input layer connects with hidden layer, which in turn connects to the output layer. The connections are weighted and weights are optimized using a learning rule.

There are many learning rules that are used with neural network:

a) least mean square;
b) gradient descent;
c) newton’s rule;
d) conjugate gradient etc.

The learning rules can be used in conjunction with backpropgation error method. The learning rule is used to calculate the error at the output unit. This error is backpropagated to all the units such that the error at each unit is proportional to the contribution of that unit towards total error at the output unit.  The errors at each unit are then used to optimize the weight at each connection. Figure 1 displays the structure of a simple neural network model for better understanding..

![](nn1.png){width=50% height=50% }

#Neural Network in R

We will fit a neural network model in R. In this example we use a subset of cereal dataset shared by Carnegie Mellon University (CMU).

The details of the dataset are on the following link: http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html. 

# Read the Data
```{r}
# Read the Data
data = read.csv("cereals.csv", header=T)
data
```
The assignment of the data to training and test set is done using random sampling.(We perform random sampling on R using sample ( ) function. We have used set.seed( ) to generate same random sample everytime and   maintain consistency)

Training set is used to find the relationship between dependent and independent variables while the test set assesses the performance of the model. We use 60% of the dataset as training set.

# Random sampling
```{r}
samplesize = 0.60 * nrow(data)
set.seed(80)
index = sample( seq_len ( nrow ( data ) ), size = samplesize )
```
Exercise: Run the code using others values for the samplesize, tip: use 0.80 and 0.10. Check out the results. 


We divide the data into training and test set. 
# Create training and test set
```{r}
datatrain = data[ index, ]
datatest = data[ -index, ]
```

The next step is used to scale the cereal dataset. The scaling of data is very important because otherwise a variable may have large impact on the prediction variable only because of its scale. Using unscaled may lead to meaningless results. The common techniques to scale data are: min-max normalization, Z-score normalization, median and MAD, and tan-h estimators. The min-max normalization transforms the data into a common range, thus removing the scaling effect from all the variables. 

## Scale data for neural network
```{r}
max = apply(data , 2 , max)
min = apply(data, 2 , min)
scaled = as.data.frame(scale(data, center = min, scale = max - min))
```

The objective of our net is to predict the rating of the cereals considering as input the calories, proteins, fat, etc. present in the grains. 

We use rating as the dependent variable and calories, proteins, fat, sodium and fiber as the independent variables. 

## Modeling the Neural Network 
First, let's install the packets! 
```{r}
# install library
install.packages("neuralnet")
```
```{r}
# load library
library(neuralnet)
```

# Making the training and test sets
```{r}
trainNN = scaled[index , ]
testNN = scaled[-index , ]
```

Look the first example of the Test Set. Change 1 by another number, for example 2, 10, etc.  
```{r}
testNN[1,] 
```
Let's train the net !

Neuralnet is used to train neural networks using backpropagation, resilient backpropagation (RPROP)
with (Riedmiller, 1994) or without weight backtracking (Riedmiller and Braun, 1993) or the modified
globally convergent version (GRPROP) by Anastasiadis et al. (2005). The function allows
flexible settings through custom-choice of error and activation function. Furthermore the calculation
of generalized weights (Intrator O. and Intrator N., 1993) is implemented.

Usage: neuralnet(formula, data, hidden = 1, threshold = 0.01,
stepmax = 1e+05, rep = 1, startweights = NULL,
learningrate.limit = NULL,
learningrate.factor = list(minus = 0.5, plus = 1.2),
learningrate=NULL, lifesign = "none",
lifesign.step = 1000, algorithm = "rprop+",
err.fct = "sse", act.fct = "logistic",
linear.output = TRUE, exclude = NULL,
constant.weights = NULL, likelihood = FALSE)

Arguments: 
formula:  a symbolic description of the model to be fitted.
data:  a data frame containing the variables specified in formula.
hidden: a vector of integers specifying the number of hidden neurons (vertices) in each layer.
threshold: a numeric value specifying the threshold for the partial derivatives of the error function as stopping criteria.
stepmax: the maximum steps for the training of the neural network. Reaching this maximum leads to a stop of the neural network’s training process.
rep: the number of repetitions for the neural network’s training.
startweights: a vector containing starting values for the weights. The weights will not be randomly initialized.
learningrate.limit: a vector or a list containing the lowest and highest limit for the learning rate. Used only for RPROP and GRPROP.
learningrate: a numeric value specifying the learning rate used by traditional backpropagation. Used only for traditional backpropagation.
algorithm: a string containing the algorithm type to calculate the neural network. The following types are possible: ’backprop’, ’rprop+’, ’rprop-’, ’sag’, or ’slr’. ’backprop’ refers to backpropagation, ’rprop+’ and ’rprop-’ refer to the resilient backpropagation with and without weight backtracking, while ’sag’ and ’slr’ induce the usage of the modified globally convergent algorithm (grprop). See Details for more information.
err.fct:  a differentiable function that is used for the calculation of the error. Alternatively,the strings ’sse’ and ’ce’ which stand for the sum of squared errors and the cross-entropy can be used.
act.fct:  a differentiable function that is used for smoothing the result of the cross product of the covariate or neurons and the weights. Additionally the strings, ’logistic’ and ’tanh’ are possible for the logistic function and tangent hyperbolicus.
linear.output: logical. If act.fct should not be applied to the output neurons set linear output to TRUE, otherwise to FALSE.
exclude: a vector or a matrix specifying the weights, that are excluded from the calculation. If given as a vector, the exact positions of the weights must be known. A matrix with n-rows and 3 columns will exclude n weights, where the first column stands for the layer, the second column for the input neuron and the third column for the output neuron of the weight. 
constant.weights: a vector specifying the values of the weights that are excluded from the training process and treated as fix.
likelihood: logical. If the error function is equal to the negative log-likelihood function, the information criteria AIC and BIC will be calculated. Furthermore the usage of confidence.interval is meaningfull.
```{r}
# fit neural network
set.seed(2)
NN = neuralnet(rating ~ calories + protein + fat + sodium + fiber, trainNN, hidden = c(1,1,8), linear.output = T )
```
Exercise1: 
1. Execute the code above using (linear.output = F) and check for possible changes in the accuracy of the net.  
2. Change the network parameters (number of the layers and the the neurons in them) and check if something different has occurred with the accuracy of the network! 
Tips: Try c=(4,5), 5, c=(1,0), etc. 

```{r}
# plot neural network
plot(NN)
```

Let's test the trainned net. Here we are using the first sample of the Test Set (testNN), try with others. Ex: 2, 5, 6, etc.
For example: predict_testNN = compute(NN, testNN[3,c(1:5)])
```{r}
predict_testNN = compute(NN, testNN[1,c(1:5)])
```

Check out the output! 
$net.result must be near of the (rating) output in the testNN[1,] (See above!)
```{r}
predict_testNN
```
```{r}
## Prediction using neural network

predict_testNN = compute(NN, testNN[,c(1:5)])
predict_testNN = (predict_testNN$net.result * (max(data$rating) - min(data$rating))) + min(data$rating)

plot(datatest$rating, predict_testNN, col='blue', pch=16, ylab = "predicted rating NN", xlab = "real rating")

abline(0,1)

# Calculate Root Mean Square Error (RMSE)
RMSE.NN = (sum((datatest$rating - predict_testNN)^2) / nrow(datatest)) ^ 0.5
```
```{r}
## Cross validation of neural network model

# install relevant libraries
install.packages("boot")
install.packages("plyr")

# Load libraries
library(boot)
library(plyr)

# Initialize variables
set.seed(50)
k = 100
RMSE.NN = NULL

List = list( )

# Fit neural network model within nested for loop
for(j in 10:65){
    for (i in 1:k) {
        index = sample(1:nrow(data),j )

        trainNN = scaled[index,]
        testNN = scaled[-index,]
        datatest = data[-index,]

        NN = neuralnet(rating ~ calories + protein + fat + sodium + fiber, trainNN, hidden = 3, linear.output= T)
        predict_testNN = compute(NN,testNN[,c(1:5)])
        predict_testNN = (predict_testNN$net.result*(max(data$rating)-min(data$rating)))+min(data$rating)

        RMSE.NN [i]<- (sum((datatest$rating - predict_testNN)^2)/nrow(datatest))^0.5
    }
    List[[j]] = RMSE.NN
}

Matrix.RMSE = do.call(cbind, List)
```


```{r}
## Prepare boxplot
boxplot(Matrix.RMSE[,56], ylab = "RMSE", main = "RMSE BoxPlot (length of traning set = 65)")
```

```{r}
## Variation of median RMSE 
install.packages("matrixStats")
library(matrixStats)

med = colMedians(Matrix.RMSE)

X = seq(10,65)

plot (med~X, type = "l", xlab = "length of training set", ylab = "median RMSE", main = "Variation of RMSE with length of training set")
```

```{r}
## Prediction using neural network

predict_testNN = compute(NN, testNN[,c(1:5)])
predict_testNN = (predict_testNN$net.result * (max(data$rating) - min(data$rating))) + min(data$rating)

plot(datatest$rating, predict_testNN, col='blue', pch=16, ylab = "predicted rating NN", xlab = "real rating")

abline(0,1)

# Calculate Root Mean Square Error (RMSE)
RMSE.NN = (sum((datatest$rating - predict_testNN)^2) / nrow(datatest)) ^ 0.5
```

-----------------------------------------------------------------------------------------------------------------------------------
# Another Example 

#Solving the XOR-problem with a Neural Net  

-----------------------------------------------------------------------------------------------------------------------------------
```{r}
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
xor.data

print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=c(2,1), rep=1,  linear.output=FALSE))
plot(net.xor, rep="best")
```
 Exercise: 
  1. Try to improve the accuracy of the XOR-Neural Net (Tip: set different values for (hidden) and (rep) parameters!)
  For example: hidden = c(3,2), 5, 8, etc; rep = 10, 1000, etc. 

